{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b771b5bd-1b67-418b-b7b4-8175339d6d17",
   "metadata": {},
   "source": [
    "# Лабораторная работа 8\n",
    "- **Дедлайн**: 11.06.2023, 23:59\n",
    "- **Максимум баллов**: 10\n",
    "\n",
    "В первой главе данного курса вам было предложено с помощью фреймворка sklearn обучить модель машинного обучения для задачи бинарной классификации на наборе данных Titanic. Давайте попробуем выполнить ту же самую задачу с помощью фреймворка глубокого обучения TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497cd50-6330-4112-b0ca-601a460ac85c",
   "metadata": {},
   "source": [
    "Давайте убедимся, что необходимые нам для этой работы библиотеки уже установлены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f49680-bb94-4eae-a006-e69e3ea31ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (1.22.4)\n",
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (4.9.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (2.12.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (4.23.2)\n",
      "Requirement already satisfied: toml in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (1.13.1)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: promise in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: array-record in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (1.3.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.10)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.12.0)\n",
      "Requirement already satisfied: zipp in c:\\programdata\\anaconda3\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.7.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.9)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.19.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bahra\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->tensorflow_datasets) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow_datasets tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0f099-9236-41e6-a985-69758dbae2dd",
   "metadata": {},
   "source": [
    "# Описание набора данных\n",
    "В данной работе целью исследования будет набор данных Titanic. Данный набор содержит информацию о различных пассажирах корабля \"Титаник\", затонувшего в ночь на 15-е апреля 1912 года. Некоторое количество пассажиров спаслось, чему способствовало множество различных факторов, включая их пол, возраст, на какой палубе находилась их кабина, социальный статус, и т.д. Мы предлагаем вам натренировать нейронную сеть для бинарной классификации, способную предсказывать вероятность спасения человека на основе его данных.  \n",
    "\n",
    "Набор данных состоит из различных признаков, описывающих информацию о пассажирах. Каждая строка таблицы - отдельный пассажир, вся информация о нем содержится в его строке. В столбце survived находится бинарная метка (0 или 1), означающая, спасся ли человек с корабля (1) или нет (0). Вашей задачей является изучить набор данных, корректно определить решаемую задачу, а затем построить модель, которая будет способна предсказывать вероятность спасения и посчитать F1-метрику на тестовом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361dfa0d-46c0-4abb-bcad-0e301290bc34",
   "metadata": {},
   "source": [
    "Для загрузки набора данных мы снова будем использовать библиотеку tensorflow_datasets. \n",
    "\n",
    "Импортируем данную библиотеку, а затем с помощью нее загрузим набор данных \"titanic\". В процессе загрузки сразу разделим его на тренировочную, валидационную и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1a6d18-31e6-41f0-9e12-019c41fe828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int32.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds_train_tf, ds_validation_tf, ds_test_tf = tfds.load(\n",
    "    name='titanic',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff561ff-9506-41c1-bf34-159eb302d97e",
   "metadata": {},
   "source": [
    "По умолчанию библиотека загружает данные в формате TensorFlow Dataset. Этот формат позволяет оперировать данными на жестком диске без предварительной загрузки их в память, что позволят обучать модели на данных, превышающих размер оперативной памяти вашего устройства.\n",
    "\n",
    "Набор данных Titanic является достаточно небольшим набором и мы уверены, что оперативной памяти вашего устройства хватит для хранения его целиком, поэтому это в данный момент не играет решающей роли. Однако, вам будет полезно научиться работать с ним.\n",
    "\n",
    "Примеры использования и различных функций этого формата данных вы можете найти в документации или Jupyter Notebook'е данного курса, описывающем работу с библиотекой Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710905d4-a25f-42d4-94a2-6d75369ad559",
   "metadata": {},
   "source": [
    "Давайте выведем первую строку тренировочного набора данных для ознакомления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88731941-8e7f-421f-a2bc-4c12fba81f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x={'age': <tf.Tensor: shape=(), dtype=float32, numpy=30.0>, 'boat': <tf.Tensor: shape=(), dtype=string, numpy=b'Unknown'>, 'body': <tf.Tensor: shape=(), dtype=int32, numpy=-1>, 'cabin': <tf.Tensor: shape=(), dtype=string, numpy=b'Unknown'>, 'embarked': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'fare': <tf.Tensor: shape=(), dtype=float32, numpy=13.0>, 'home.dest': <tf.Tensor: shape=(), dtype=string, numpy=b'Sarnia, ON'>, 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'McCrie, Mr. James Matthew'>, 'parch': <tf.Tensor: shape=(), dtype=int32, numpy=0>, 'pclass': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'sex': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sibsp': <tf.Tensor: shape=(), dtype=int32, numpy=0>, 'ticket': <tf.Tensor: shape=(), dtype=string, numpy=b'233478'>}\n",
      "y=<tf.Tensor: shape=(), dtype=int64, numpy=0>\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds_train_tf.take(1):\n",
    "    print(f\"{x=}\")\n",
    "    print(f\"{y=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895b942-7e3a-49db-bc31-9b4d75312103",
   "metadata": {},
   "source": [
    "Как можно заметить, формат текущего датасета представляет собой кортеж из двух элементов - признаков и метки класса. Признаки организованы как словарь с ключами (названиями признаков) и значениями признаков. Данный формат можно использовать напрямую для метода model.fit, однако признаки, несомненно, требуют предобработки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73348261-bfdd-4a04-8372-7906d79600a9",
   "metadata": {},
   "source": [
    "Итак, к настоящему моменту мы загрузили набор данных, разделили его на тренировочную, валидационную и тестовую выборку и готовы приступить к решению задачи. Вашим финальным заданием является получение максимального значения F1-метрики на тестовом наборе данных, используя все возможности по изучению и визуализации данных, предобработке данных, а также нейронную сеть, выполненную с помощью фреймворка TensorFlow (и его части - Keras).  \n",
    "\n",
    "Для того, чтобы напомнить процесс подготовки данных и обучения модели, мы поместили несколько ячеек с комментариями ниже в предполагаемом порядке. Однако, вы можете использовать любой удобный вам процесс.  \n",
    "\n",
    "В процессе исследования и тренировки запрещено использовать данных из ds_test кроме как для финальной оценки F1 метрики. Данная лабораторная работа служит не для проверки ваших навыков, а для того, чтобы помочь вам вспомнить основы машинного обучения и работы с соответствующими библиотеками. Мы просим вас попытаться решить задачу самостоятельно, не используя доступную информацию из ds_test набора и не пытаясь подогнать модель под идеальную работу с конкретно данным набором, а также просим не использовать внешние источники данных и информацию из соответствующих соревнований на платформе Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f6838-33d5-42ee-b661-d9dfcb992eee",
   "metadata": {},
   "source": [
    "Вы можете найти для себя полезным пройти снова ноутбук с описанием работы фреймворка Keras и информацией о том, как его использовать. Также по данному фреймворку доступны многочисленные туториалы и примеры на его сайте. Напомним вам, что задача представляет собой бинарную классификацию, поэтому вам необходимо выбрать правильное количество нейронов для последнего слоя и функцию активации.\n",
    "\n",
    "Успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5e5da",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e4dd3",
   "metadata": {},
   "source": [
    "* 1. Предобработка данных: заполнение пропущенных мест, приведение столбцов к единому виду, удаление nan/infinity значений\n",
    "* 2. Инженерия признаков: генерация или выделение наиболее интересных признаков, удаление ненужных\n",
    "* 3. Нормализация данных относительно среднего и медианы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc15004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(features, label):\n",
    "    # Fill in missing values\n",
    "    age_mean = tf.reduce_mean(features['age'])\n",
    "    fare_mean = tf.reduce_mean(features['fare'])\n",
    "    features['age'] = tf.where(tf.math.is_nan(features['age']), age_mean, features['age'])\n",
    "    features['fare'] = tf.where(tf.math.is_nan(features['fare']), fare_mean, features['fare'])\n",
    "    \n",
    "    # Create new feature: total family members on board\n",
    "    features['family_members'] = features['sibsp'] + features['parch']\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    sex = tf.one_hot(tf.cast(tf.as_string(features['sex']) == 'male', tf.int32), depth=2)\n",
    "    pclass = tf.one_hot(features['pclass'] - 1, depth=3)\n",
    "    \n",
    "    # Extract title from name and one-hot encode it\n",
    "    title = tf.strings.split(features['name'], ',')[1]\n",
    "    title = tf.strings.split(title)[0]\n",
    "    title = tf.where(title == 'Miss.', 'Ms.', title)\n",
    "    title = tf.where(title == 'Mlle.', 'Ms.', title)\n",
    "    title = tf.where(title == 'Mme.', 'Mrs.', title)\n",
    "    unique_titles = ['Mr.', 'Mrs.', 'Ms.', 'Master.', 'Dr.', 'Rev.']\n",
    "    title = tf.one_hot(tf.argmax(tf.stack([tf.cast(title == t, tf.int32) for t in unique_titles])), depth=len(unique_titles))\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    epsilon = 1e-8\n",
    "    age = (features['age'] - age_mean) / (tf.math.reduce_std(features['age']) + epsilon)\n",
    "    fare = (features['fare'] - fare_mean) / (tf.math.reduce_std(features['fare']) + epsilon)\n",
    "    \n",
    "    # Expand dimensions of scalar features\n",
    "    age = tf.expand_dims(age, axis=-1)\n",
    "    fare = tf.expand_dims(fare, axis=-1)\n",
    "    \n",
    "    # Concatenate all features into a single tensor\n",
    "    inputs = tf.concat([sex, pclass, title, age, fare], axis=-1)\n",
    "    \n",
    "    return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01b2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the data\n",
    "ds_train_tf = ds_train_tf.map(preprocess)\n",
    "ds_validation_tf = ds_validation_tf.map(preprocess)\n",
    "ds_test_tf = ds_test_tf.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec90fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the first few examples in the training dataset\n",
    "for features, label in ds_train_tf.take(1):\n",
    "    print(f'Features: {features}')\n",
    "    print(f'Label: {label}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bebf07",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c4861",
   "metadata": {},
   "source": [
    "* 1. Выбор модели или ансамбля моделей\n",
    "* 2. Поиск оптимальных гиперпараметров на validation наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e486419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(62, activation='relu'),\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2819cea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 1s 8ms/step - loss: 0.5262 - accuracy: 0.7564 - val_loss: 0.4069 - val_accuracy: 0.8321\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4670 - accuracy: 0.7870 - val_loss: 0.3995 - val_accuracy: 0.8168\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7861 - val_loss: 0.4016 - val_accuracy: 0.8092\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.7908 - val_loss: 0.3930 - val_accuracy: 0.8321\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.7841 - val_loss: 0.3949 - val_accuracy: 0.8321\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.7803 - val_loss: 0.3982 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.7775 - val_loss: 0.4025 - val_accuracy: 0.8092\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4595 - accuracy: 0.7784 - val_loss: 0.3999 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7784 - val_loss: 0.4101 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7822 - val_loss: 0.4144 - val_accuracy: 0.8321\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5291 - accuracy: 0.7469 - val_loss: 0.3941 - val_accuracy: 0.8321\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4681 - accuracy: 0.7927 - val_loss: 0.3984 - val_accuracy: 0.8397\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.7889 - val_loss: 0.3940 - val_accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4619 - accuracy: 0.7870 - val_loss: 0.3933 - val_accuracy: 0.8397\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.7794 - val_loss: 0.4121 - val_accuracy: 0.8397\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.7889 - val_loss: 0.4005 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.7966 - val_loss: 0.4063 - val_accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4626 - accuracy: 0.7927 - val_loss: 0.4038 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7832 - val_loss: 0.4145 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7822 - val_loss: 0.4196 - val_accuracy: 0.8397\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5485 - accuracy: 0.7354 - val_loss: 0.4088 - val_accuracy: 0.8321\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4774 - accuracy: 0.7880 - val_loss: 0.3923 - val_accuracy: 0.8397\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4621 - accuracy: 0.7870 - val_loss: 0.3941 - val_accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.7803 - val_loss: 0.3889 - val_accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.7899 - val_loss: 0.3989 - val_accuracy: 0.8092\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 0.7794 - val_loss: 0.3952 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7765 - val_loss: 0.4085 - val_accuracy: 0.8244\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7813 - val_loss: 0.3982 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7765 - val_loss: 0.4163 - val_accuracy: 0.8015\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7755 - val_loss: 0.4223 - val_accuracy: 0.8015\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5322 - accuracy: 0.7402 - val_loss: 0.4497 - val_accuracy: 0.8244\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4766 - accuracy: 0.7775 - val_loss: 0.4311 - val_accuracy: 0.8397\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.7822 - val_loss: 0.4189 - val_accuracy: 0.8397\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.7794 - val_loss: 0.4131 - val_accuracy: 0.8397\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.7755 - val_loss: 0.4124 - val_accuracy: 0.8397\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4538 - accuracy: 0.7775 - val_loss: 0.4111 - val_accuracy: 0.8321\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.7775 - val_loss: 0.4103 - val_accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.7784 - val_loss: 0.4112 - val_accuracy: 0.8092\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4519 - accuracy: 0.7765 - val_loss: 0.4111 - val_accuracy: 0.8092\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.7765 - val_loss: 0.4111 - val_accuracy: 0.8092\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5212 - accuracy: 0.7564 - val_loss: 0.4201 - val_accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.7861 - val_loss: 0.4121 - val_accuracy: 0.8168\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7861 - val_loss: 0.4122 - val_accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.7861 - val_loss: 0.4085 - val_accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.7851 - val_loss: 0.4086 - val_accuracy: 0.8397\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4538 - accuracy: 0.7870 - val_loss: 0.4074 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.7813 - val_loss: 0.4079 - val_accuracy: 0.8168\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7851 - val_loss: 0.4056 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4538 - accuracy: 0.7784 - val_loss: 0.4052 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.7822 - val_loss: 0.4063 - val_accuracy: 0.8397\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.5083 - accuracy: 0.7383 - val_loss: 0.4282 - val_accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.7880 - val_loss: 0.4090 - val_accuracy: 0.8168\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4575 - accuracy: 0.7870 - val_loss: 0.4071 - val_accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7870 - val_loss: 0.4070 - val_accuracy: 0.8397\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4538 - accuracy: 0.7851 - val_loss: 0.4065 - val_accuracy: 0.8397\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4529 - accuracy: 0.7851 - val_loss: 0.4061 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4524 - accuracy: 0.7851 - val_loss: 0.4055 - val_accuracy: 0.8397\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4522 - accuracy: 0.7832 - val_loss: 0.4046 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.7870 - val_loss: 0.4049 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7880 - val_loss: 0.4049 - val_accuracy: 0.8397\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6224 - accuracy: 0.7106 - val_loss: 0.6250 - val_accuracy: 0.6870\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7287 - val_loss: 0.5866 - val_accuracy: 0.6947\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7545 - val_loss: 0.5445 - val_accuracy: 0.8092\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.7832 - val_loss: 0.5116 - val_accuracy: 0.8321\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7775 - val_loss: 0.4862 - val_accuracy: 0.8321\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4896 - accuracy: 0.7775 - val_loss: 0.4690 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7832 - val_loss: 0.4581 - val_accuracy: 0.8397\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4765 - accuracy: 0.7832 - val_loss: 0.4508 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.7832 - val_loss: 0.4457 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4698 - accuracy: 0.7832 - val_loss: 0.4421 - val_accuracy: 0.8397\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6361 - accuracy: 0.7039 - val_loss: 0.5942 - val_accuracy: 0.7939\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5594 - accuracy: 0.7794 - val_loss: 0.5530 - val_accuracy: 0.8244\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.7794 - val_loss: 0.5170 - val_accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.5039 - accuracy: 0.7746 - val_loss: 0.4880 - val_accuracy: 0.8244\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.7784 - val_loss: 0.4698 - val_accuracy: 0.8321\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4829 - accuracy: 0.7822 - val_loss: 0.4585 - val_accuracy: 0.8321\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4776 - accuracy: 0.7822 - val_loss: 0.4514 - val_accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4734 - accuracy: 0.7822 - val_loss: 0.4461 - val_accuracy: 0.8321\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 0.7822 - val_loss: 0.4420 - val_accuracy: 0.8321\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.7822 - val_loss: 0.4388 - val_accuracy: 0.8397\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 1s 6ms/step - loss: 0.6166 - accuracy: 0.6915 - val_loss: 0.5895 - val_accuracy: 0.7557\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.7794 - val_loss: 0.5201 - val_accuracy: 0.8015\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.5019 - accuracy: 0.7841 - val_loss: 0.4768 - val_accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7832 - val_loss: 0.4555 - val_accuracy: 0.8397\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.7832 - val_loss: 0.4439 - val_accuracy: 0.8397\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4698 - accuracy: 0.7832 - val_loss: 0.4375 - val_accuracy: 0.8397\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.7832 - val_loss: 0.4327 - val_accuracy: 0.8397\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4621 - accuracy: 0.7832 - val_loss: 0.4293 - val_accuracy: 0.8397\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4592 - accuracy: 0.7832 - val_loss: 0.4264 - val_accuracy: 0.8397\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7832 - val_loss: 0.4241 - val_accuracy: 0.8397\n",
      "Best validation accuracy: 0.8396946787834167\n",
      "Best learning rate: 0.1\n",
      "Best number of units: 32\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "num_units = [32, 64, 128]\n",
    "\n",
    "best_val_acc = 0\n",
    "best_lr = None\n",
    "best_units = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for units in num_units:\n",
    "        # Define the model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        # Compile the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # Train the model\n",
    "        history = model.fit(ds_train_tf.batch(32), validation_data=ds_validation_tf.batch(32), epochs=10)\n",
    "        # Get the best validation accuracy\n",
    "        val_acc = max(history.history['val_accuracy'])\n",
    "        # Update the best hyperparameters if necessary\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_lr = lr\n",
    "            best_units = units\n",
    "\n",
    "print(f'Best validation accuracy: {best_val_acc}')\n",
    "print(f'Best learning rate: {best_lr}')\n",
    "print(f'Best number of units: {best_units}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674be58",
   "metadata": {},
   "source": [
    "# Оценка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef1c9b",
   "metadata": {},
   "source": [
    "* 1. Получение F1-метрики на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc197c6a-18ac-4b35-9b48-0707e0ccff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4468 - accuracy: 0.8015\n",
      "Test accuracy: 0.8015267252922058\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(ds_test_tf.batch(32))\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38cf839f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1000us/step\n",
      "Test F1 score: 0.7292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Calculate and print out the F1 score on the test set\n",
    "y_true = tf.stack([y for x, y in ds_test_tf], axis=0).numpy()\n",
    "y_pred = tf.round(model.predict(ds_test_tf.batch(32))).numpy()\n",
    "test_f1 = f1_score(y_true, y_pred)\n",
    "print(f'Test F1 score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09489b",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "* The model is working well.\n",
    "* We got good results from f1, but it is still not enough.\n",
    "* As we can see the accurary is not increasing fron 0.7832 which means the model has reached its maximum capacity.\n",
    "* Another maybe there are still noise in the dataset or contains errors which why the model is accuracy is not increasing.\n",
    "* Another possible reason for the model's accuracy not increasing is overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
