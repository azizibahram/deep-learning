{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZgbDF3u_OHl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Многослойный перцептрон\n",
    "\n",
    "В этом блокноте мы реализуем возможность построения полносвязной многослойной нейронной сети при помощи `numpy`. Сначала загрузим требующиеся библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "05WLHrGG_Jxu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrF6iqhS5HEy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Немного про то, как действуют слои"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvc4016BnaLC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "На данный момент мы намеренно не будем расширять матрицу входных данных значениями $-1$, как мы это делали в однослойной сети, и опишем действие слоя нейронной сети несколько иначе. \n",
    "\n",
    "Пусть нам дан на вход некоторый набор данных $X$ размера $[n \\times p]$, состоящий из $n$ объектов, каждый из которых характеризуется $p$ фичами. Действие любого скрытого слоя нейронной сети можно разбить на два этапа. Первый этап — это действие сумматора, производится оно следующим образом:\n",
    "$$\n",
    "Y_1 = X\\cdot W_0 - b_0,\n",
    "$$\n",
    "где $W_0$ — матрица размера $[p \\times n_1]$, где $n_1$ — количество нейронов следующего слоя, $b_0$ — вектор смещений каждого нейрона (по сути — матрица размера $[n \\times n_1]$, чьи элементы, находящиеся в одном столбце, одинаковы). Итого, на выходе мы получаем для каждого объекта $n_1$ новый признак. Но это не все, второй этап — применение некоторой функции активации $\\varphi_0$ поэлементно ко всем элементам матрицы $Y_1$, полученной этапом ранее:\n",
    "$$\n",
    "Z_1 = \\varphi_0(Y_1).\n",
    "$$\n",
    "Полученная матрица и является входной матрицей для следующего слоя нейронов. \n",
    "\n",
    "Таким образом, имея $(k - 1)$ построенный слой, мы можем построить $k$-ый слой при помощи двух операций:\n",
    "$$\n",
    "Y_k = Z_{k - 1}W_{k-1} - b_{k-1}, \\quad Z_k = \\varphi_{k-1}(Y_k).\n",
    "$$\n",
    "Последний слой обычно выделяют особо, так как этот слой — выходной, и в зависимости от решаемой задачи его выход может разниться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlQolHDp5TIk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Выходной слой при классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Dw2Rgt25ve5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы, опять-таки, будем решать задачу классификации, так что введем еще раз уже известные обозначения.\n",
    "\n",
    "Пусть\n",
    "$$\n",
    "x_i = (x_i^1, ..., x_i^{p}), \\quad i \\in \\{1, 2, ..., n\\},\n",
    "$$\n",
    "— $i$-ый тренировочный объект, $y_i$ — числовая метка класса $i$-ого объекта,\n",
    "$$\n",
    "w^j = (w_1^j, ..., w_{p}^j)^T, \\quad j \\in \\{1, 2, ..., m\\},\n",
    "$$\n",
    "— веса $j$-ого нейрона — столбцы матрицы $W$, $b^j$ — смещение $j$-ого нейрона — столбцы матрицы $b$.\n",
    "\n",
    "Напишем интересующую нас функцию потерь, $y = \\{y_1, ..., y_n\\}$:\n",
    "$$\n",
    "Loss(X, W, y) = -\\frac{1}{n} \\sum\\limits_{i = 1}^n \\ln \\frac{\\exp(x_i \\cdot w^{y_i} - b^{y_i})}{\\sum\\limits_{j = 1}^m\\exp(x_i \\cdot w^j - b^j)} + \\lambda R(w) = -\\frac{1}{n}\\sum\\limits_{i = 1}^n \\left((x_i \\cdot w^{y_i} - b^{y_i}) - \\ln \\sum\\limits_{j = 1}^m\\exp(x_i \\cdot w^j - b^j)\\right) + \\lambda  R(w),\n",
    "$$\n",
    "где \n",
    "$$\n",
    "x_i \\cdot w^j = \\sum\\limits_{k = 1}^px_i^kw^j_k\n",
    "$$\n",
    "— скалярное произведение соответсвующих векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxfchwYU4muJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для обучения сети нам понадобится градиент этой функции, вычислим его.\n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w^{y'}_p} = -\\frac{1}{n} \\sum\\limits_{i = 1}^n \\left( x_i^p[y_i = y'] - x_i^p \\frac{exp(x_i \\cdot w^{y'})}{\\sum\\limits_{j = 1}^m \\exp(x_i \\cdot w^j)}\\right) + \\lambda  \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "В таком виде градиент использовать неудобно и вычислительно неэффективно. Можно заметить, что в матричном виде он переписывается следующим образом:\n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w} = -\\frac{1}{n}X^T\\left(M - P\\right) + \\lambda \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "Поясним входящие в последнее выражение объекты.  Как мы уже отметили, до применения функции активации выходы нейронов после «сумматора» для набора данных $X$ могут быть получены следущим образом:\n",
    "$$\n",
    "Outs = X \\cdot W - b.\n",
    "$$\n",
    "Матрица $Outs$ имеет размер $[n \\times m]$ и построчно содержит значения выходов каждого из $m$ нейронов для соответсвующего объекта подаваемых данных. Тогда матрица $P$ — это матрица `softmax`-ов для каждого нейрона, на пересечении $i$-ой строки и $t$-ого столбца которой стоят значения\n",
    "$$\n",
    "\\frac{\\exp(x_i \\cdot w^t)}{\\sum\\limits_{j = 1}^m \\exp(x_i \\cdot w^j)}, \\quad i \\in \\{1, 2, ..., n\\}, \\quad t \\in \\{1, 2, ..., m\\},\n",
    "$$\n",
    "$M$ — матрица размера $[n \\times m]$ — разреженная матрица `one_hot` кодированных откликов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG6-e4X_5Zya",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Снова про регуляризацию\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAO4FCvt5zz-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Теперь про регуляризацию. В случае $l_p$, $p \\in \\{1, 2\\}$, регуляризатор имеет вид:\n",
    "$$\n",
    "R_p(W) = \\sum\\limits_{i = 1}^p\\sum\\limits_{j = 1}^m |w_i^j|^p,\n",
    "$$\n",
    "поэтому в матричном виде производная (или градиент) может быть записана так:\n",
    "$$\n",
    "\\frac{\\partial R_2}{\\partial w} = 2\\lambda W, \\quad \\frac{\\partial R_1}{\\partial \\omega} = \\lambda \\operatorname{sign}W.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg6sR3Lp52m8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Теперь о матричном дифференцировании"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FvErTc-6BTf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Матричные операции очень неплохо реализованы в питоне, поэтому именно им и стоит отдать предпочтение при реализации шага градиентного спуска для поиска параметров слоев. Мы уже поняли, что если выходы последнего слоя завязаны на функции потерь, описанной выше, то \n",
    "$$\n",
    "\\frac{\\partial Loss(X, W, y)}{\\partial w} = -\\frac{1}{n}X^T\\left(M - P\\right) + \\lambda \\frac{\\partial R}{\\partial w}.\n",
    "$$\n",
    "Это реализовано в `__get_grad`. В общем случае,\n",
    "$$\n",
    "Z = \\varphi(XW - b), \\quad \\frac{\\partial Z}{\\partial W} = X^T\\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)}, \\quad \\frac{\\partial Z}{db} = - \\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)}.\n",
    "$$\n",
    "Немного деликатнее оказывается вопрос вычисления более «глубоких» производных, ведь параметры предыдущих слоев находятся внутри $X$, а $X$ получается в результате применения функции активации предыдущего слоя. Поятно, что если\n",
    "$$\n",
    "X = \\psi(\\widetilde X \\widetilde W - \\widetilde b),\n",
    "$$\n",
    "то\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial \\widetilde W} = \\widetilde X^T\\left(\\left(\\frac{\\partial(\\varphi(XW - b))}{\\partial(XW - b)} W^T\\right) \\odot \\frac{\\partial \\psi(X \\widetilde W - \\widetilde b)}{\\partial (X \\widetilde W - \\widetilde b)}\\right),\n",
    "$$\n",
    "и так далее. Это правило цепочки реализовано в самом конце метода `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTN3U0Zv_PIw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Программная реализация и тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork:\n",
    "    __REGULARIZATION_GRAD = {None: lambda _w: 0, \"l1\": lambda _w: np.sign(_w), \"l2\": lambda _w: 2*_w}\n",
    "    __REGULARIZATION_FUNC = {None: lambda _w: 0, \"l1\": lambda _w: np.abs(_w), \"l2\": lambda _w: _w ** 2}\n",
    "    __LOSS = 0\n",
    "    # создание нейронной сети: alpha — скорость обучения (шаг градиентного спуска), reg_type — тип регуляризации (если есть), lambda — параметр регуляризации; слои будут храниться в списке layers\n",
    "    def __init__(self, alpha=0.01, reg_type=None, lambda_=0, loss='MSE'):\n",
    "        self.__layers = list()\n",
    "        self.__alpha = alpha\n",
    "        self.__reg_type = reg_type\n",
    "        self.__lambda = lambda_\n",
    "        self.__loss = loss\n",
    "\n",
    "    # метод, позволяющий добавить новый слой: указываем правильные размеры слоя, название функции активации, class_number — количество классов в случае использования Sotmax'а на последнем слое, параметр a — параметр LeakyReLU\n",
    "    def add_layer(self, size: tuple, activation_func: str, class_number=0, a=0):\n",
    "        if not self.__layers or self.__layers[-1].size[1] == size[0]:\n",
    "            self.__layers.append(FullyConnectedLayer(size, activation_func, class_number, a))\n",
    "        else:\n",
    "            raise Exception(\"Wrong size of the layer!\")\n",
    "\n",
    "    def change_alpha(self, alpha):\n",
    "        self.__alpha = alpha\n",
    "\n",
    "    def get_loss(self):\n",
    "        return FullyConnectedNetwork.__LOSS\n",
    "\n",
    "    # метод, выдающий предсказания для заданного набора данных после обучения модели\n",
    "    def predict(self, data):\n",
    "        current_output = data\n",
    "        for layer in self.__layers[:-1]:\n",
    "            current_output, _ = layer.forward(current_output, None)\n",
    "        # отдельно обрабатываем последний слой\n",
    "        layer_weights, layer_biases = self.__layers[-1].get_weights()\n",
    "        current_output = np.matmul(current_output, layer_weights) - layer_biases\n",
    "        return current_output\n",
    "\n",
    "    def score(self, data, answers):\n",
    "        predictions = self.predict(data)\n",
    "        if self.__loss == 'MSE':\n",
    "            return ((predictions - answers) ** 2).mean()\n",
    "        elif self.__loss == 'MAE':\n",
    "            return np.abs(predictions - answers).mean()\n",
    "\n",
    "    def fit(self, data, answers):\n",
    "        # выход входного слоя совпадает с фичами входных данных\n",
    "        layer_outputs = [data]\n",
    "        current_output = layer_outputs[0]\n",
    "        grads = []\n",
    "        # forward pass и вычисление градиентов функций активации\n",
    "        for layer in self.__layers:\n",
    "            current_output, gradient = layer.forward(current_output, answers)\n",
    "            layer_outputs.append(current_output)\n",
    "            grads.append(gradient)\n",
    "        # для вычисления градиентов по правилу цепочки, удобно развернуть массив\n",
    "        grads = grads[::-1]\n",
    "        # для градиента параметров самого первого слоя, умножаем на «производную» независимой переменной\n",
    "        grads.append(1)\n",
    "        \n",
    "        if self.__loss == 'MSE':\n",
    "            current_gradient = 2 * (layer_outputs[-1] - answers) / len(answers)\n",
    "            FullyConnectedNetwork.__LOSS = ((layer_outputs[-1] - answers) ** 2).mean()\n",
    "        elif self.__loss == 'MAE':\n",
    "            current_gradient = np.sign(layer_outputs[-1] - answers) / len(answers)\n",
    "            FullyConnectedNetwork.__LOSS = np.abs(layer_outputs[-1] - answers).mean()\n",
    "        \n",
    "        for i, layer in enumerate(self.__layers[::-1]):\n",
    "            layer_weights, layer_biases = layer.get_weights()\n",
    "            FullyConnectedNetwork.__LOSS += self.__lambda * (np.sum(FullyConnectedNetwork.__REGULARIZATION_FUNC[self.__reg_type](layer_weights) + FullyConnectedNetwork.__REGULARIZATION_FUNC[self.__reg_type](layer_biases)))\n",
    "            # вычисление градиента параметров W слоя layer\n",
    "            d_weights = np.matmul(layer_outputs[-2 - i].T, current_gradient)\n",
    "            # вычисление градиента параметров db слоя layer\n",
    "            d_bias = -np.matmul(np.ones(layer_outputs[-2 - i].shape[0]), current_gradient) / layer_outputs[-2 - i].shape[0]\n",
    "            # выполнение шага градиентного спуска\n",
    "            layer.update_weights(self.__alpha * (d_weights + self.__lambda * FullyConnectedNetwork.__REGULARIZATION_GRAD[self.__reg_type](layer_weights)) , self.__alpha * (d_bias + self.__lambda * FullyConnectedNetwork.__REGULARIZATION_GRAD[self.__reg_type](layer_biases)))\n",
    "            # правило цепочки\n",
    "            current_gradient = np.matmul(current_gradient, layer_weights.T) * grads[i + 1]\n",
    "            \n",
    "class FullyConnectedLayer:\n",
    "    # мы предполагаем что реализованы следующие функции активации \n",
    "    __ACTIVATION_FUNCTIONS={'ReLU':{'func':lambda a,x : np.maximum(x , 0),'derivative':lambda a,x : np.where(x >= 0 , 1 , 0)},\n",
    "                            'LReLU':{'func':lambda a,x : np.where(x >= 0 , x , a*x),'derivative':lambda a,x : np.where(x >= 0 , 1 , a)},\n",
    "                            'None':{'func':lambda a,x : x , 'derivative':lambda a,x : 1},\n",
    "                            'Sigmoid':{'func':lambda a,x : np.exp(x)/(1+np.exp(x)),'derivative':lambda a,x : np.exp(x)/(1+np.exp(x))**2},\n",
    "                            }\n",
    "    # создание нового слоя задание размеров слоя \n",
    "    def __init__(self,size : tuple , activation_func : str , class_number=0,a=0):\n",
    "        self.size=size\n",
    "        self.__weights=np.random.random((size[0],size[1]))-0.5\n",
    "        self.__bias=np.random.random((1,size[1]))-0.5\n",
    "        self.__a=a\n",
    "        if activation_func in FullyConnectedLayer. __ACTIVATION_FUNCTIONS.keys():\n",
    "             self. __activation_func=activation_func \n",
    "        else:\n",
    "             raise Exception(\"No such activation function!\")\n",
    "        \n",
    "    # метод возвращающий значения весов : веса и смещения \n",
    "    def get_weights(self):\n",
    "         return self. __weights,self. __bias\n",
    "    \n",
    "    # метод модифицирующий веса после градиентного шага \n",
    "    def update_weights(self, d_weights, d_biases):\n",
    "#         print(f\"d_weights shape: {d_weights.shape}, __weights shape: {self.__weights.shape}\")\n",
    "        self.__weights -= d_weights\n",
    "        self.__bias -= d_biases\n",
    "\n",
    "    \n",
    "    # метод возвращающий градиент \n",
    "    def __get_grad(self,data):\n",
    "        \n",
    "         return FullyConnectedLayer. __ACTIVATION_FUNCTIONS[self. __activation_func]['derivative'](self. __a,data)\n",
    "\n",
    "    \n",
    "    # проход по слою с вычислением градиента функции активации на текущей итерации и текущем наборе данных \n",
    "    def forward(self,data,_):\n",
    "         matrix_pass=np.matmul(data,self.get_weights()[0])-self.get_weights()[1]\n",
    "         activation=FullyConnectedLayer. __ACTIVATION_FUNCTIONS[self. __activation_func]['func'](self. __a,matrix_pass)\n",
    "         gradient=self. __get_grad(matrix_pass)\n",
    "         return activation , gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the y_train and y_test arrays\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing our model with loss function = mse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  2379.403023035973 Test MSE: 4115.0184527865285 Current loss: (99, 2385.857819234478)\n",
      "Training MSE:  2272.6723066843274 Test MSE: 4154.712319958246 Current loss: (199, 2275.05771605398)\n",
      "Training MSE:  2251.147841261612 Test MSE: 4168.514981005207 Current loss: (299, 2253.004200698077)\n",
      "Training MSE:  2244.8439701183133 Test MSE: 4173.18087361424 Current loss: (399, 2246.5727013002443)\n",
      "Training MSE:  2242.858187548063 Test MSE: 4174.577369645317 Current loss: (499, 2244.5490129277323)\n",
      "Training MSE:  2242.1874788518326 Test MSE: 4175.053748311188 Current loss: (599, 2243.865307699329)\n",
      "Training MSE:  2241.9638552179267 Test MSE: 4175.217783188558 Current loss: (699, 2243.6374322552183)\n",
      "Training MSE:  2241.889485607004 Test MSE: 4175.27243649514 Current loss: (799, 2243.5615655576767)\n",
      "Training MSE:  2241.864692552184 Test MSE: 4175.290828842827 Current loss: (899, 2243.536292431074)\n",
      "Training MSE:  2241.8564293063355 Test MSE: 4175.2968823555575 Current loss: (999, 2243.527867201933)\n",
      "Training MSE:  2241.853673427052 Test MSE: 4175.298903380633 Current loss: (1099, 2243.525058283661)\n",
      "Training MSE:  2241.852755076879 Test MSE: 4175.299563971137 Current loss: (1199, 2243.524121759533)\n",
      "Training MSE:  2241.852448981186 Test MSE: 4175.299788910557 Current loss: (1299, 2243.523809890893)\n",
      "Training MSE:  2241.8523469348424 Test MSE: 4175.299863228059 Current loss: (1399, 2243.5237058073394)\n",
      "Training MSE:  2241.8523129439222 Test MSE: 4175.2998883975615 Current loss: (1499, 2243.523671107326)\n",
      "Training MSE:  2241.8523016024674 Test MSE: 4175.299896741769 Current loss: (1599, 2243.5236595522733)\n",
      "Training MSE:  2241.852297825225 Test MSE: 4175.299899497785 Current loss: (1699, 2243.523655697907)\n",
      "Training MSE:  2241.8522965656266 Test MSE: 4175.299900418491 Current loss: (1799, 2243.523654413108)\n",
      "Training MSE:  2241.8522961455415 Test MSE: 4175.299900726779 Current loss: (1899, 2243.5236539851817)\n",
      "Training MSE:  2241.8522960054365 Test MSE: 4175.29990083062 Current loss: (1999, 2243.5236538424983)\n",
      "Training MSE:  2241.852295958807 Test MSE: 4175.299900864933 Current loss: (2099, 2243.523653794945)\n",
      "Training MSE:  2241.8522959432516 Test MSE: 4175.29990087629 Current loss: (2199, 2243.5236537790884)\n",
      "Training MSE:  2241.8522959380707 Test MSE: 4175.299900880156 Current loss: (2299, 2243.523653773805)\n",
      "Training MSE:  2241.852295936342 Test MSE: 4175.2999008814295 Current loss: (2399, 2243.5236537720425)\n",
      "Training MSE:  2241.8522959357656 Test MSE: 4175.299900881853 Current loss: (2499, 2243.523653771455)\n",
      "Training MSE:  2241.8522959355732 Test MSE: 4175.299900881996 Current loss: (2599, 2243.5236537712594)\n",
      "Training MSE:  2241.85229593551 Test MSE: 4175.2999008820425 Current loss: (2699, 2243.5236537711944)\n",
      "Training MSE:  2241.852295935488 Test MSE: 4175.299900882058 Current loss: (2799, 2243.523653771172)\n",
      "Training MSE:  2241.8522959354823 Test MSE: 4175.299900882062 Current loss: (2899, 2243.523653771166)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (2999, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3099, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3199, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3299, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3399, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3499, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3599, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3699, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3799, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3899, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (3999, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4099, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4199, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4299, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4399, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4499, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4599, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4699, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4799, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4899, 2243.523653771165)\n",
      "Training MSE:  2241.852295935481 Test MSE: 4175.299900882062 Current loss: (4999, 2243.523653771165)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the neural network\n",
    "NN = FullyConnectedNetwork(alpha=0.001, reg_type='l2', lambda_=0.002, loss='MSE')\n",
    "loss = []\n",
    "NN.add_layer((10, 121), 'ReLU')\n",
    "NN.add_layer((121, 1), 'None')\n",
    "alpha = 0.001\n",
    "\n",
    "for ep in range(5000):\n",
    "    NN.fit(X_train,y_train)\n",
    "    loss.append((ep , NN.get_loss()))\n",
    "    if (ep+1)%100==0:\n",
    "         alpha=alpha/3\n",
    "         NN.change_alpha(alpha)\n",
    "         print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can the results are not bad, but MSE is still bigger than expected**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try MAE as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  149.59781128082656 Test MSE: 152.63945920923723 Current loss: (99, 150.04599370037653)\n",
      "Training MSE:  148.73116389684404 Test MSE: 151.8548474951322 Current loss: (199, 149.16268391572197)\n",
      "Training MSE:  148.4412361529903 Test MSE: 151.5926177313558 Current loss: (299, 148.86721384802527)\n",
      "Training MSE:  148.34446526570594 Test MSE: 151.50511611894035 Current loss: (399, 148.76859607097748)\n",
      "Training MSE:  148.31219326944966 Test MSE: 151.47593796094964 Current loss: (499, 148.73570840939422)\n",
      "Training MSE:  148.3014345138757 Test MSE: 151.4662105381317 Current loss: (599, 148.72474444046637)\n",
      "Training MSE:  148.29784808036322 Test MSE: 151.46296793537797 Current loss: (699, 148.72108960257776)\n",
      "Training MSE:  148.2966525823085 Test MSE: 151.46188705349192 Current loss: (799, 148.71987130307068)\n",
      "Training MSE:  148.29625408070933 Test MSE: 151.46152675793994 Current loss: (899, 148.71946520098797)\n",
      "Training MSE:  148.29612124659315 Test MSE: 151.46140665924588 Current loss: (999, 148.7193298333774)\n",
      "Training MSE:  148.29607696852668 Test MSE: 151.46136662632824 Current loss: (1099, 148.7192847108128)\n",
      "Training MSE:  148.2960622091681 Test MSE: 151.46135328202018 Current loss: (1199, 148.71926966995483)\n",
      "Training MSE:  148.29605728938157 Test MSE: 151.46134883391724 Current loss: (1299, 148.71926465633518)\n",
      "Training MSE:  148.29605564945268 Test MSE: 151.46134735121623 Current loss: (1399, 148.7192629851286)\n",
      "Training MSE:  148.2960551028097 Test MSE: 151.46134685698257 Current loss: (1499, 148.71926242805972)\n",
      "Training MSE:  148.2960549205954 Test MSE: 151.461346692238 Current loss: (1599, 148.71926224237006)\n",
      "Training MSE:  148.29605485985726 Test MSE: 151.46134663732312 Current loss: (1699, 148.71926218047355)\n",
      "Training MSE:  148.29605483961123 Test MSE: 151.46134661901817 Current loss: (1799, 148.71926215984135)\n",
      "Training MSE:  148.29605483286255 Test MSE: 151.46134661291657 Current loss: (1899, 148.71926215296395)\n",
      "Training MSE:  148.29605483061297 Test MSE: 151.46134661088266 Current loss: (1999, 148.71926215067148)\n",
      "Training MSE:  148.29605482986312 Test MSE: 151.46134661020469 Current loss: (2099, 148.7192621499073)\n",
      "Training MSE:  148.29605482961318 Test MSE: 151.4613466099787 Current loss: (2199, 148.7192621496526)\n",
      "Training MSE:  148.29605482952988 Test MSE: 151.4613466099034 Current loss: (2299, 148.7192621495677)\n",
      "Training MSE:  148.29605482950208 Test MSE: 151.46134660987826 Current loss: (2399, 148.71926214953945)\n",
      "Training MSE:  148.29605482949285 Test MSE: 151.4613466098699 Current loss: (2499, 148.71926214952998)\n",
      "Training MSE:  148.29605482948978 Test MSE: 151.46134660986712 Current loss: (2599, 148.71926214952683)\n",
      "Training MSE:  148.29605482948872 Test MSE: 151.46134660986618 Current loss: (2699, 148.71926214952578)\n",
      "Training MSE:  148.2960548294884 Test MSE: 151.46134660986587 Current loss: (2799, 148.71926214952546)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (2899, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (2999, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3099, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3199, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3299, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3399, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3499, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3599, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3699, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3799, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3899, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (3999, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4099, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4199, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4299, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4399, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4499, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4599, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4699, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4799, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4899, 148.71926214952538)\n",
      "Training MSE:  148.29605482948833 Test MSE: 151.4613466098658 Current loss: (4999, 148.71926214952538)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the neural network\n",
    "NN = FullyConnectedNetwork(alpha=0.001, reg_type='l2', lambda_=0.002, loss='MAE')\n",
    "loss = []\n",
    "NN.add_layer((10, 121), 'ReLU')\n",
    "NN.add_layer((121, 1), 'None')\n",
    "alpha = 0.001\n",
    "\n",
    "for ep in range(5000):\n",
    "    NN.fit(X_train,y_train)\n",
    "    loss.append((ep , NN.get_loss()))\n",
    "    if (ep+1)%100==0:\n",
    "         alpha=alpha/3\n",
    "         NN.change_alpha(alpha)\n",
    "         print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model is working pretty good with MAE as loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try more layers with loss function = MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  2454.613849524079 Test MSE: 3886.3479911144764 Current loss: (99, 2469.718888480301)\n",
      "Training MSE:  2330.233135208905 Test MSE: 3993.5628776567423 Current loss: (199, 2341.584806899577)\n",
      "Training MSE:  2298.2253772380855 Test MSE: 4028.010377560326 Current loss: (299, 2308.8769933603367)\n",
      "Training MSE:  2287.951042762909 Test MSE: 4040.0074977252248 Current loss: (399, 2298.386899859456)\n",
      "Training MSE:  2284.647350952331 Test MSE: 4043.714249341708 Current loss: (499, 2295.0165716515835)\n",
      "Training MSE:  2283.5458783253075 Test MSE: 4044.930821929991 Current loss: (599, 2293.8934652525063)\n",
      "Training MSE:  2283.1795689198066 Test MSE: 4045.333848311961 Current loss: (699, 2293.5197717688056)\n",
      "Training MSE:  2283.0575727627192 Test MSE: 4045.4683672780025 Current loss: (799, 2293.3953928195756)\n",
      "Training MSE:  2283.016928550407 Test MSE: 4045.513243974025 Current loss: (899, 2293.353928508789)\n",
      "Training MSE:  2283.0033728481776 Test MSE: 4045.5282047806572 Current loss: (999, 2293.340104519011)\n",
      "Training MSE:  2282.9988556189605 Test MSE: 4045.5331990415234 Current loss: (1099, 2293.3354968251033)\n",
      "Training MSE:  2282.9973497723545 Test MSE: 4045.5348569967946 Current loss: (1199, 2293.333960824749)\n",
      "Training MSE:  2282.996847789072 Test MSE: 4045.5354090689452 Current loss: (1299, 2293.333448978758)\n",
      "Training MSE:  2282.996680477273 Test MSE: 4045.5355940019185 Current loss: (1399, 2293.3332783311334)\n",
      "Training MSE:  2282.9966247124003 Test MSE: 4045.53565543309 Current loss: (1499, 2293.3332214467446)\n",
      "Training MSE:  2282.9966061195423 Test MSE: 4045.535675969578 Current loss: (1599, 2293.3332024859774)\n",
      "Training MSE:  2282.996599921399 Test MSE: 4045.5356827823207 Current loss: (1699, 2293.3331961665244)\n",
      "Training MSE:  2282.9965978561777 Test MSE: 4045.535685066974 Current loss: (1799, 2293.3331940597495)\n",
      "Training MSE:  2282.9965971676233 Test MSE: 4045.535685828048 Current loss: (1899, 2293.3331933575696)\n",
      "Training MSE:  2282.9965969381074 Test MSE: 4045.535686081346 Current loss: (1999, 2293.3331931234534)\n",
      "Training MSE:  2282.996596861612 Test MSE: 4045.5356861656464 Current loss: (2099, 2293.333193045421)\n",
      "Training MSE:  2282.9965968361084 Test MSE: 4045.535686193846 Current loss: (2199, 2293.333193019414)\n",
      "Training MSE:  2282.9965968276083 Test MSE: 4045.535686203225 Current loss: (2299, 2293.333193010744)\n",
      "Training MSE:  2282.996596824775 Test MSE: 4045.535686206347 Current loss: (2399, 2293.3331930078534)\n",
      "Training MSE:  2282.996596823831 Test MSE: 4045.535686207385 Current loss: (2499, 2293.3331930068907)\n",
      "Training MSE:  2282.996596823518 Test MSE: 4045.5356862077238 Current loss: (2599, 2293.333193006571)\n",
      "Training MSE:  2282.9965968234146 Test MSE: 4045.5356862078365 Current loss: (2699, 2293.333193006466)\n",
      "Training MSE:  2282.996596823385 Test MSE: 4045.535686207865 Current loss: (2799, 2293.3331930064355)\n",
      "Training MSE:  2282.9965968233787 Test MSE: 4045.535686207865 Current loss: (2899, 2293.333193006429)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (2999, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3099, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3199, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3299, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3399, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3499, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3599, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3699, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3799, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3899, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (3999, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4099, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4199, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4299, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4399, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4499, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4599, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4699, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4799, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4899, 2293.333193006428)\n",
      "Training MSE:  2282.9965968233773 Test MSE: 4045.535686207864 Current loss: (4999, 2293.333193006428)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the neural network\n",
    "NN = FullyConnectedNetwork(alpha=0.0001, reg_type='l2', lambda_=0.002, loss='MSE')\n",
    "loss = []\n",
    "NN.add_layer((10, 121), 'ReLU')\n",
    "NN.add_layer((121, 242), 'ReLU')\n",
    "NN.add_layer((242, 1), 'None')\n",
    "alpha = 0.0001\n",
    "\n",
    "for ep in range(5000):\n",
    "    NN.fit(X_train,y_train)\n",
    "    loss.append((ep , NN.get_loss()))\n",
    "    if (ep+1)%100==0:\n",
    "         alpha=alpha/3\n",
    "         NN.change_alpha(alpha)\n",
    "         print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see after adding more layers, our model is getting better. But, in some point MSE is not decreasing, which means it is not good enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try adding more layer with MAE as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  96.89311907424351 Test MSE: 104.28146781000703 Current loss: (99, 107.652898365492)\n",
      "Training MSE:  81.53342291254367 Test MSE: 87.5352553422601 Current loss: (199, 91.9793803571235)\n",
      "Training MSE:  76.73068869913284 Test MSE: 82.48235463288118 Current loss: (299, 87.08088738399185)\n",
      "Training MSE:  75.1709104407356 Test MSE: 80.80420219998486 Current loss: (399, 85.4904923200086)\n",
      "Training MSE:  74.64685748849014 Test MSE: 80.23967151714773 Current loss: (499, 84.95662850181071)\n",
      "Training MSE:  74.47082874352877 Test MSE: 80.05012785882492 Current loss: (599, 84.77727683741165)\n",
      "Training MSE:  74.41200236387033 Test MSE: 79.98679173193892 Current loss: (699, 84.71733694329428)\n",
      "Training MSE:  74.39237543239598 Test MSE: 79.96566267964101 Current loss: (799, 84.69733820078832)\n",
      "Training MSE:  74.38583125100466 Test MSE: 79.95861730680069 Current loss: (899, 84.69066999630009)\n",
      "Training MSE:  74.38364963487723 Test MSE: 79.95626863692468 Current loss: (999, 84.68844703033456)\n",
      "Training MSE:  74.38292240634887 Test MSE: 79.95548572429664 Current loss: (1099, 84.68770601762128)\n",
      "Training MSE:  74.38267999426682 Test MSE: 79.9552247509017 Current loss: (1199, 84.68745901071028)\n",
      "Training MSE:  74.38259918995362 Test MSE: 79.95513775949019 Current loss: (1299, 84.68737667477626)\n",
      "Training MSE:  74.38257225515079 Test MSE: 79.95510876232193 Current loss: (1399, 84.6873492294319)\n",
      "Training MSE:  74.38256327687965 Test MSE: 79.9550990965957 Current loss: (1499, 84.6873400809801)\n",
      "Training MSE:  74.38256028412218 Test MSE: 79.95509587468653 Current loss: (1599, 84.68733703149574)\n",
      "Training MSE:  74.38255928653629 Test MSE: 79.95509480071676 Current loss: (1699, 84.6873360150009)\n",
      "Training MSE:  74.38255895400768 Test MSE: 79.95509444272686 Current loss: (1799, 84.68733567616928)\n",
      "Training MSE:  74.38255884316486 Test MSE: 79.95509432339693 Current loss: (1899, 84.68733556322546)\n",
      "Training MSE:  74.3825588062172 Test MSE: 79.95509428362023 Current loss: (1999, 84.68733552557747)\n",
      "Training MSE:  74.38255879390131 Test MSE: 79.95509427036133 Current loss: (2099, 84.68733551302815)\n",
      "Training MSE:  74.3825587897961 Test MSE: 79.9550942659418 Current loss: (2199, 84.68733550884514)\n",
      "Training MSE:  74.38255878842764 Test MSE: 79.95509426446854 Current loss: (2299, 84.68733550745071)\n",
      "Training MSE:  74.38255878797145 Test MSE: 79.95509426397741 Current loss: (2399, 84.68733550698592)\n",
      "Training MSE:  74.38255878781942 Test MSE: 79.95509426381375 Current loss: (2499, 84.68733550683103)\n",
      "Training MSE:  74.3825587877688 Test MSE: 79.95509426375926 Current loss: (2599, 84.68733550677943)\n",
      "Training MSE:  74.38255878775196 Test MSE: 79.95509426374122 Current loss: (2699, 84.68733550676231)\n",
      "Training MSE:  74.38255878774663 Test MSE: 79.95509426373559 Current loss: (2799, 84.68733550675687)\n",
      "Training MSE:  74.38255878774523 Test MSE: 79.95509426373407 Current loss: (2899, 84.68733550675542)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (2999, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3099, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3199, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3299, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3399, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3499, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3599, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3699, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3799, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3899, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (3999, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4099, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4199, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4299, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4399, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4499, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4599, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4699, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4799, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4899, 84.68733550675529)\n",
      "Training MSE:  74.38255878774511 Test MSE: 79.95509426373394 Current loss: (4999, 84.68733550675529)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the neural network\n",
    "NN = FullyConnectedNetwork(alpha=0.001, reg_type='l2', lambda_=0.002, loss='MAE')\n",
    "loss = []\n",
    "NN.add_layer((10, 121), 'ReLU')\n",
    "NN.add_layer((121, 242), 'ReLU')\n",
    "NN.add_layer((242, 1), 'None')\n",
    "alpha = 0.001\n",
    "\n",
    "for ep in range(5000):\n",
    "    NN.fit(X_train,y_train)\n",
    "    loss.append((ep , NN.get_loss()))\n",
    "    if (ep+1)%100==0:\n",
    "         alpha=alpha/3\n",
    "         NN.change_alpha(alpha)\n",
    "         print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results are good with MAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "icWyHthPA6Y_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NN = FullyConnectedNetwork(alpha=0.01, reg_type='l2', lambda_=0.2, loss='MSE')\n",
    "NN.add_layer((10, 1), 'ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "KfWSlVrNA_PI",
    "outputId": "4c342e0d-5bef-4417-90c5-87f627f75095",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  27011.978472190807 Test MSE: 25198.75656365621 Current loss: (99, 16959.448466006765)\n",
      "Training MSE:  26951.632710128088 Test MSE: 24709.221410212307 Current loss: (199, 16688.69964692361)\n",
      "Training MSE:  26866.863805707675 Test MSE: 24545.234010853586 Current loss: (299, 16640.105084214763)\n",
      "Training MSE:  26805.97008359929 Test MSE: 24474.673428771042 Current loss: (399, 16617.27832387571)\n",
      "Training MSE:  26764.043228636903 Test MSE: 24436.27522974749 Current loss: (499, 16604.69314295072)\n",
      "Training MSE:  26735.510226061284 Test MSE: 24412.44288023407 Current loss: (599, 16597.132607510786)\n",
      "Training MSE:  26716.184681937742 Test MSE: 24396.786706734918 Current loss: (699, 16592.43279051951)\n",
      "Training MSE:  26703.130716413914 Test MSE: 24386.27922951806 Current loss: (799, 16589.45574554663)\n",
      "Training MSE:  26694.32815072851 Test MSE: 24379.174744633518 Current loss: (899, 16587.547127375652)\n",
      "Training MSE:  26688.399133298113 Test MSE: 24374.360619001032 Current loss: (999, 16586.313670368945)\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "for ep in range(1000):\n",
    "    NN.fit(X_train,y_train)\n",
    "    loss.append((ep , NN.get_loss()))\n",
    "    if (ep+1)%100==0:\n",
    "         print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NN = FullyConnectedNetwork(alpha=0.01, reg_type='l2', lambda_=0.2)\n",
    "NN.add_layer((10, 1), 'ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  5938.675180462673 Test MSE: 5881.888649490655 Current loss: (99, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (199, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (299, 23019.65034592146)\n",
      "Training MSE:  5938.675180462673 Test MSE: 5881.888649490655 Current loss: (399, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (499, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (599, 23019.65034592146)\n",
      "Training MSE:  5938.675180462673 Test MSE: 5881.888649490655 Current loss: (699, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (799, 23019.65034592146)\n",
      "Training MSE:  5938.675180462672 Test MSE: 5881.888649490655 Current loss: (899, 23019.65034592146)\n",
      "Training MSE:  5938.675180462673 Test MSE: 5881.888649490655 Current loss: (999, 23019.65034592146)\n",
      "CPU times: total: 2.34 s\n",
      "Wall time: 3.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "batch_size = int(len(X_train) / 100)\n",
    "for ep in range(1000):\n",
    "    for _ in range(0, 100):\n",
    "        X_batch = X_train[_ * batch_size : (_ + 1) * batch_size]\n",
    "        y_batch = y_train[_ * batch_size : (_ + 1) * batch_size]\n",
    "        NN.fit(X_batch, y_batch)\n",
    "        loss.append((ep, NN.get_loss()))\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YF7-t81OuWXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NN = FullyConnectedNetwork(alpha=0.0001, reg_type='l2', lambda_=0.2)\n",
    "NN.add_layer((10, 64), 'ReLU')\n",
    "NN.add_layer((64, 121), 'ReLU')\n",
    "NN.add_layer((121, 1), 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  2575.903663757834 Test MSE: 4030.7917114064467 Current loss: (99, 2903.5960631115518)\n",
      "Training MSE:  2226.326740740151 Test MSE: 4196.602967399412 Current loss: (199, 2548.049276120995)\n",
      "Training MSE:  2082.712203844051 Test MSE: 4311.249996678754 Current loss: (299, 2402.7135582837127)\n",
      "Training MSE:  1963.8918694895606 Test MSE: 4494.668021710446 Current loss: (399, 2283.3588109423854)\n",
      "Training MSE:  1858.5700310029963 Test MSE: 4693.431115322299 Current loss: (499, 2177.843810275431)\n",
      "Training MSE:  1765.009827421931 Test MSE: 4866.764893630025 Current loss: (599, 2084.419479985919)\n",
      "Training MSE:  1675.2281367617934 Test MSE: 5036.102447716469 Current loss: (699, 1995.0337518784775)\n",
      "Training MSE:  1592.1804299462751 Test MSE: 5180.582423506888 Current loss: (799, 1912.41440150528)\n",
      "Training MSE:  1511.1711790824995 Test MSE: 5312.727075615885 Current loss: (899, 1832.1298708273011)\n",
      "Training MSE:  1436.2920859894748 Test MSE: 5432.9444388805205 Current loss: (999, 1757.8032916647821)\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "for ep in range(1000):\n",
    "    NN.fit(X_train, y_train)\n",
    "    loss.append((ep, NN.get_loss()))\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "NN = FullyConnectedNetwork(alpha=alpha, reg_type='l2', lambda_=0.02, loss='MAE')\n",
    "NN.add_layer((10, 121), 'ReLU')\n",
    "NN.add_layer((121, 1), 'None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  41.01460196351846 Test MSE: 52.69224867453433 Current loss: (9, 61.44171788382245)\n",
      "Training MSE:  38.75507722665328 Test MSE: 51.37010416336248 Current loss: (19, 63.72583966780593)\n",
      "Training MSE:  37.533718402351546 Test MSE: 51.339834493773935 Current loss: (29, 58.38319766121691)\n",
      "Training MSE:  37.354875020545045 Test MSE: 51.316298998222074 Current loss: (39, 57.801773307805064)\n",
      "Training MSE:  37.14413450785706 Test MSE: 51.14899629104934 Current loss: (49, 55.89867478383594)\n",
      "Training MSE:  36.8067225333201 Test MSE: 50.86799914335148 Current loss: (59, 57.254875510376216)\n",
      "Training MSE:  36.95178761789738 Test MSE: 50.94303821188315 Current loss: (69, 55.716665802484385)\n",
      "Training MSE:  36.41945213324147 Test MSE: 51.002608157449586 Current loss: (79, 56.53924781070435)\n",
      "Training MSE:  36.632473434292386 Test MSE: 51.35815103464751 Current loss: (89, 56.5161626437477)\n",
      "Training MSE:  36.4906271062397 Test MSE: 51.039255189928234 Current loss: (99, 55.840284531185866)\n",
      "Training MSE:  35.61836771096777 Test MSE: 51.02708746447463 Current loss: (109, 52.49178948045076)\n",
      "Training MSE:  35.43934368995904 Test MSE: 51.02682748358285 Current loss: (119, 52.992418043514306)\n",
      "Training MSE:  35.412121555106395 Test MSE: 50.98961482849705 Current loss: (129, 53.69901917960702)\n",
      "Training MSE:  35.33548531953317 Test MSE: 51.001032008905995 Current loss: (139, 52.57610174546953)\n",
      "Training MSE:  35.3504257290886 Test MSE: 51.03141513924322 Current loss: (149, 53.27674463977789)\n",
      "Training MSE:  35.25599857384446 Test MSE: 51.14082315345132 Current loss: (159, 52.929470474421905)\n",
      "Training MSE:  35.283311808621754 Test MSE: 51.029148570145225 Current loss: (169, 53.46416246858254)\n",
      "Training MSE:  35.24127624730908 Test MSE: 51.126576696131615 Current loss: (179, 53.28828164503157)\n",
      "Training MSE:  35.30650699092906 Test MSE: 51.05296704760593 Current loss: (189, 53.694074907484676)\n",
      "Training MSE:  35.25652414493174 Test MSE: 51.035686635884275 Current loss: (199, 53.76874343310347)\n",
      "Training MSE:  35.08413622011918 Test MSE: 51.10830192373492 Current loss: (209, 52.24797644154971)\n",
      "Training MSE:  35.06453493382358 Test MSE: 51.100442900901406 Current loss: (219, 52.30808596355425)\n",
      "Training MSE:  35.045875334710594 Test MSE: 51.10330948290268 Current loss: (229, 52.22523652235438)\n",
      "Training MSE:  35.03936278123321 Test MSE: 51.11231341669586 Current loss: (239, 52.248095311303686)\n",
      "Training MSE:  35.035164101403055 Test MSE: 51.10662654235717 Current loss: (249, 52.05948486499351)\n",
      "Training MSE:  35.02674905193033 Test MSE: 51.12352456910512 Current loss: (259, 52.25025580880473)\n",
      "Training MSE:  35.02466305023104 Test MSE: 51.09570682931687 Current loss: (269, 52.129625823622675)\n",
      "Training MSE:  35.02885666449754 Test MSE: 51.08694177922362 Current loss: (279, 51.945902204771045)\n",
      "Training MSE:  35.021084712862226 Test MSE: 51.10321581768473 Current loss: (289, 52.00669363268932)\n",
      "Training MSE:  35.01116367198939 Test MSE: 51.12503157375469 Current loss: (299, 51.89687055360134)\n",
      "Training MSE:  34.99122521670167 Test MSE: 51.13513424933759 Current loss: (309, 51.762727836355424)\n",
      "Training MSE:  34.986409334680864 Test MSE: 51.13783750526219 Current loss: (319, 51.75388169308249)\n",
      "Training MSE:  34.98305897454373 Test MSE: 51.14063921274771 Current loss: (329, 51.745339751618616)\n",
      "Training MSE:  34.98183248377495 Test MSE: 51.140406144482704 Current loss: (339, 51.731546294066725)\n",
      "Training MSE:  34.980540195741575 Test MSE: 51.14192673059445 Current loss: (349, 51.735009513454315)\n",
      "Training MSE:  34.98087423707338 Test MSE: 51.1382280343482 Current loss: (359, 51.74753431095184)\n",
      "Training MSE:  34.979775924333005 Test MSE: 51.14053919484519 Current loss: (369, 51.73119485977865)\n",
      "Training MSE:  34.97792566117438 Test MSE: 51.143404673763165 Current loss: (379, 51.73344671770936)\n",
      "Training MSE:  34.975783120637495 Test MSE: 51.142538832187284 Current loss: (389, 51.71153786708571)\n",
      "Training MSE:  34.975231685771625 Test MSE: 51.144054005055125 Current loss: (399, 51.70798948188863)\n",
      "Training MSE:  34.97275944687204 Test MSE: 51.14757315667436 Current loss: (409, 51.66915150919291)\n",
      "Training MSE:  34.972197078458336 Test MSE: 51.14987009475979 Current loss: (419, 51.66131674215717)\n",
      "Training MSE:  34.97150120196961 Test MSE: 51.15143187153523 Current loss: (429, 51.65666338036852)\n",
      "Training MSE:  34.97123805850251 Test MSE: 51.152098911461245 Current loss: (439, 51.656308410017374)\n",
      "Training MSE:  34.970840552324546 Test MSE: 51.15243714451749 Current loss: (449, 51.65178652463723)\n",
      "Training MSE:  34.970886106712534 Test MSE: 51.152567897073595 Current loss: (459, 51.65298407609126)\n",
      "Training MSE:  34.97023309783882 Test MSE: 51.15318547575428 Current loss: (469, 51.64993912190688)\n",
      "Training MSE:  34.97016410856779 Test MSE: 51.151529644687805 Current loss: (479, 51.64957435143969)\n",
      "Training MSE:  34.97004522854389 Test MSE: 51.152429502480544 Current loss: (489, 51.64998052758652)\n",
      "Training MSE:  34.96997816556555 Test MSE: 51.15269919672482 Current loss: (499, 51.650622649046845)\n",
      "Training MSE:  34.969341456332515 Test MSE: 51.15373264554316 Current loss: (509, 51.63963704235815)\n",
      "Training MSE:  34.96920157963237 Test MSE: 51.15395822732441 Current loss: (519, 51.637532909013714)\n",
      "Training MSE:  34.969058148336 Test MSE: 51.15442906050789 Current loss: (529, 51.63628583364175)\n",
      "Training MSE:  34.9689967312385 Test MSE: 51.15447042616025 Current loss: (539, 51.63606954561739)\n",
      "Training MSE:  34.96891510294403 Test MSE: 51.15448917001038 Current loss: (549, 51.634437050980125)\n",
      "Training MSE:  34.968892648187534 Test MSE: 51.15455832090249 Current loss: (559, 51.635129402337554)\n",
      "Training MSE:  34.96875530393947 Test MSE: 51.15482804100807 Current loss: (569, 51.63419448625815)\n",
      "Training MSE:  34.968816946245525 Test MSE: 51.15450139000976 Current loss: (579, 51.635560178037174)\n",
      "Training MSE:  34.96876392170455 Test MSE: 51.154723868028384 Current loss: (589, 51.63466339918574)\n",
      "Training MSE:  34.96872299574081 Test MSE: 51.154580970796765 Current loss: (599, 51.634718898962404)\n",
      "Training MSE:  34.96858708897843 Test MSE: 51.154743571749556 Current loss: (609, 51.63310585193993)\n",
      "Training MSE:  34.96852676888614 Test MSE: 51.15485887801342 Current loss: (619, 51.63232410107773)\n",
      "Training MSE:  34.96852456346709 Test MSE: 51.154941463945455 Current loss: (629, 51.632072691799166)\n",
      "Training MSE:  34.96849486582026 Test MSE: 51.154974157619506 Current loss: (639, 51.63166390217611)\n",
      "Training MSE:  34.9684893857903 Test MSE: 51.15502458592196 Current loss: (649, 51.63182436848333)\n",
      "Training MSE:  34.96848630182046 Test MSE: 51.1549933025482 Current loss: (659, 51.63185442228581)\n",
      "Training MSE:  34.968472544024756 Test MSE: 51.154951779994526 Current loss: (669, 51.63208475043728)\n",
      "Training MSE:  34.96846770111706 Test MSE: 51.15497958343897 Current loss: (679, 51.63167004920659)\n",
      "Training MSE:  34.96845762529109 Test MSE: 51.154984792745566 Current loss: (689, 51.63186204630402)\n",
      "Training MSE:  34.96844699854491 Test MSE: 51.15503546443401 Current loss: (699, 51.631788128857806)\n",
      "Training MSE:  34.96843073354441 Test MSE: 51.155039634004815 Current loss: (709, 51.63141952311908)\n",
      "Training MSE:  34.968419491517714 Test MSE: 51.155059378807536 Current loss: (719, 51.63129956376072)\n",
      "Training MSE:  34.968415289699806 Test MSE: 51.155071025699144 Current loss: (729, 51.63124780333149)\n",
      "Training MSE:  34.9684148238139 Test MSE: 51.15508307433815 Current loss: (739, 51.63121430902081)\n",
      "Training MSE:  34.96841156351969 Test MSE: 51.15508243232403 Current loss: (749, 51.631184598639436)\n",
      "Training MSE:  34.96840936463521 Test MSE: 51.15508368450716 Current loss: (759, 51.63120472716945)\n",
      "Training MSE:  34.968409185816434 Test MSE: 51.15508788777333 Current loss: (769, 51.6311610153058)\n",
      "Training MSE:  34.968409358398475 Test MSE: 51.15508594547662 Current loss: (779, 51.631158285108654)\n",
      "Training MSE:  34.96840265948381 Test MSE: 51.15508584169169 Current loss: (789, 51.63113885838752)\n",
      "Training MSE:  34.968403914474145 Test MSE: 51.155091798973835 Current loss: (799, 51.6311655952357)\n",
      "Training MSE:  34.96839931720076 Test MSE: 51.155093439706526 Current loss: (809, 51.63109784927049)\n",
      "Training MSE:  34.9683983880766 Test MSE: 51.15509822031272 Current loss: (819, 51.631076683434806)\n",
      "Training MSE:  34.96839745983119 Test MSE: 51.1550998731824 Current loss: (829, 51.631064661799016)\n",
      "Training MSE:  34.96839669578015 Test MSE: 51.15510151022941 Current loss: (839, 51.63104802089976)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:  34.96839653493826 Test MSE: 51.15510169912867 Current loss: (849, 51.63104890001145)\n",
      "Training MSE:  34.96839591418831 Test MSE: 51.15510274618357 Current loss: (859, 51.6310399057693)\n",
      "Training MSE:  34.968396281864216 Test MSE: 51.15510057863594 Current loss: (869, 51.631047121045505)\n",
      "Training MSE:  34.96839500962412 Test MSE: 51.15510225081531 Current loss: (879, 51.63104147544359)\n",
      "Training MSE:  34.968394981899905 Test MSE: 51.155102370968095 Current loss: (889, 51.6310463042057)\n",
      "Training MSE:  34.96839485423723 Test MSE: 51.15510255158005 Current loss: (899, 51.631044760061485)\n",
      "Training MSE:  34.96839401167373 Test MSE: 51.15510340305966 Current loss: (909, 51.63102881778509)\n",
      "Training MSE:  34.9683935510444 Test MSE: 51.15510490936404 Current loss: (919, 51.63102228477488)\n",
      "Training MSE:  34.968393421895414 Test MSE: 51.155105339720976 Current loss: (929, 51.6310225685682)\n",
      "Training MSE:  34.9683933598427 Test MSE: 51.15510554145513 Current loss: (939, 51.6310219231037)\n",
      "Training MSE:  34.968393446759016 Test MSE: 51.15510527872274 Current loss: (949, 51.6310224389782)\n",
      "Training MSE:  34.96839320422643 Test MSE: 51.155105487485116 Current loss: (959, 51.6310221230234)\n",
      "Training MSE:  34.96839321454292 Test MSE: 51.155105474721495 Current loss: (969, 51.63102181885455)\n",
      "Training MSE:  34.96839308825113 Test MSE: 51.15510589058927 Current loss: (979, 51.631020444592004)\n",
      "Training MSE:  34.96839307369001 Test MSE: 51.15510558468094 Current loss: (989, 51.631021003636626)\n",
      "Training MSE:  34.968393076708104 Test MSE: 51.155105516774434 Current loss: (999, 51.631020708519536)\n",
      "CPU times: total: 1.81 s\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss = []\n",
    "batch_size = int(len(X_train) / 100)\n",
    "alpha = 0.01\n",
    "for ep in range(1000):\n",
    "    for _ in range(0, 100):\n",
    "        X_batch = X_train[_ * batch_size : (_ + 1) * batch_size]\n",
    "        y_batch = y_train[_ * batch_size : (_ + 1) * batch_size]\n",
    "        NN.fit(X_batch, y_batch)\n",
    "        loss.append((ep, NN.get_loss()))\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        print('Training MSE: ', NN.score(X_train,y_train), 'Test MSE:', NN.score(X_test,y_test), 'Current loss:', loss[-1])\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        alpha = alpha / 5\n",
    "        NN.change_alpha(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.968393076708104"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# In Conclusion\n",
    "* AS we can see the best loss function is MAE for this model and dataset.\n",
    "* AS we add more layers and ep our model gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multilayer_fully_connected_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
